{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "# Create a handler to output log messages to the console\n",
    "handler = logging.StreamHandler()\n",
    "# Create a formatter to format log messages\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from jsonpath_ng import parse\n",
    "\n",
    "SEARCHXNG_URL = \"http://searxng.cloud.internal/search\"\n",
    "CRAWL4AI_URL = \"http://crawl.cloud.internal/md\"\n",
    "\n",
    "# \"Passive\" functions (external systems)\n",
    "def find_web_sources(query:str, max_search_urls=1):\n",
    "    \"\"\"Find web pages related to a search query\"\"\"\n",
    "    url = SEARCHXNG_URL\n",
    "    url_query = {'q': query, 'format': 'json', 'language': 'en-US'}\n",
    "    response = requests.get(url, params=url_query)\n",
    "    search_results = response.json().get('results', [])\n",
    "    # Sort by score\n",
    "    search_results_sorted = sorted(search_results, key=lambda x: x['score'], reverse=True)\n",
    "    jsonpath_expression = parse('$.[*].url')\n",
    "    urls = jsonpath_expression.find(search_results_sorted[:max_search_urls])\n",
    "    urls = [url.value for url in urls]\n",
    "    return urls\n",
    "\n",
    "def visit_web_page(url:str):\n",
    "    \"\"\"Get the contents of a web page\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            CRAWL4AI_URL,\n",
    "            json={\n",
    "                \"url\": url,\n",
    "                \"f\": \"fit\",\n",
    "                \"q\": None,\n",
    "                \"c\": \"0\"\n",
    "            }\n",
    "        )\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        data = response.json()\n",
    "        return data\n",
    "    except requests.RequestException as e:\n",
    "        return {\"url\": url, \"markdown\": str(e), \"is_fail\":True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29baa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_llms import Llama4Maverick\n",
    "from lmflux.flow import (\n",
    "    new_toolbox,\n",
    "    create_agent,\n",
    "    tool\n",
    ")\n",
    "from lmflux.agents.sessions import Session\n",
    "from lmflux.agents.structure import Agent\n",
    "from lmflux.core.components import (SystemPrompt, TemplatedPrompt, Message)\n",
    "\n",
    "# == Agent Definitions == \n",
    "# -- 1. \"Expert\" maker agent --\n",
    "expert_maker_llm = Llama4Maverick(SystemPrompt('general.user_preference'))\n",
    "\n",
    "# Act function\n",
    "def expert_maker_act_function(agent:Agent, session:Session):\n",
    "    agent.reset_agent_state()\n",
    "    message = TemplatedPrompt(\"expert_maker.instruct\", \"user\").get_message(\n",
    "        session.context_as_dict()\n",
    "    )\n",
    "    response = agent.conversate(message, session)\n",
    "    session.context.set(\"persona\", response.content)\n",
    "    agent.log_agent_step(session, \"has defined the persona\", [response])\n",
    "        \n",
    "\n",
    "agent_expert_maker = (\n",
    "    create_agent(expert_maker_llm, \"expert_maker\")\n",
    "        .with_act(expert_maker_act_function)\n",
    "        .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d7a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 2. \"Expert\" plan maker agent -- \n",
    "planner_llm = Llama4Maverick(SystemPrompt('general.role_play'))\n",
    "\n",
    "# Act function\n",
    "def expert_planner_act_function(agent:Agent, session:Session):\n",
    "    agent.reset_agent_state()\n",
    "    message = TemplatedPrompt(\"expert_planner.instruct\", \"user\").get_message(\n",
    "        session.context_as_dict()\n",
    "    )\n",
    "    response = agent.conversate(message, session)\n",
    "    session.context.set(\"research_plan\", response.content)\n",
    "    agent.log_agent_step(session, \"has defined the plan\", [response])\n",
    "\n",
    "agent_planner = (\n",
    "    create_agent(planner_llm, \"planner\")\n",
    "        .with_act(expert_planner_act_function)\n",
    "        .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34496594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 3a. \"Text Compressor\" agent\n",
    "system_prompt = SystemPrompt('text_compressor.system')\n",
    "reader_llm = Llama4Maverick(system_prompt)\n",
    "\n",
    "# Act function\n",
    "def text_compressor_act_function(agent:Agent, session:Session):\n",
    "    agent.reset_agent_state()\n",
    "    message = TemplatedPrompt(\"text_compressor.instruct\", \"user\").get_message(\n",
    "        session.context_as_dict()\n",
    "    )\n",
    "    response = agent.conversate(message, session)\n",
    "    session.context.set(\"compressed_text\", response.content)\n",
    "    agent.log_agent_step(session, \"has compressed a text\", [response], print_full_message=False)\n",
    "\n",
    "agent_text_compressor = (\n",
    "    create_agent(reader_llm, \"text_compressor\")\n",
    "        .with_act(text_compressor_act_function)\n",
    "        .build()\n",
    ")\n",
    "\n",
    "# Auxiliary call (the agent is defined \"outside the scope\" of the other guys)\n",
    "def llm_text_compress(text: str, source:str, attention_instructions:str) -> dict:\n",
    "    transient_session = Session()\n",
    "    transient_session.context.set(\"web_page_result\", text+f'\\nSOURCE:{source}')\n",
    "    transient_session.context.set(\"important_details\", attention_instructions)\n",
    "    agent_text_compressor.act(transient_session)\n",
    "    compressed_text = transient_session.context.get('compressed_text')\n",
    "    return {\n",
    "        \"compressed_text\": compressed_text,\n",
    "        \"source\": source\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- 3b. \"Researcher\" agent\n",
    "@tool\n",
    "def get_information(search_queries: list[str], attention_instructions:str):\n",
    "    \"\"\"\n",
    "    This tool calls for a downstream executor to fetch relevant information.\n",
    "        - search_queries: Use this parameter to specify all the queries that need to be responded to execute a plan.call_id\n",
    "        - attention_instructions: Use these instructions to refine what you wish to respond with this query. \n",
    "    \"\"\"\n",
    "    \n",
    "    if type(search_queries) != list:\n",
    "        raise ValueError(\"Not able to run, incorrect type\")\n",
    "    \n",
    "    logger.info(f\"Search algorithm called with {search_queries}\")\n",
    "    results = {}\n",
    "    for query in search_queries:\n",
    "        logger.info(f\"Search algorithm searching for: {query}\")\n",
    "        web_pages = find_web_sources(query)\n",
    "        first_page = web_pages[-1]\n",
    "        data = visit_web_page(first_page)\n",
    "        llm_summary = llm_text_compress(data.get(\"markdown\"), first_page, attention_instructions)\n",
    "        results[query] = {\"llm_summary\": llm_summary, \"url\": data.get(\"url\")}\n",
    "    return results\n",
    "\n",
    "def tool_agg(agent: Agent, tool_call, result, session: Session):\n",
    "    # Add to the \"researched_information\" pile\n",
    "    agent.log_agent_step(session, \"Agent got refined data back\", [])\n",
    "    session.context.set_cumulative(\"researched_information\", result)\n",
    "    session.context.set(\"researcher__tool_called\", True)\n",
    "\n",
    "def researcher_act_function(agent:Agent, session:Session):\n",
    "    first_query = True\n",
    "    agent.reset_agent_state()\n",
    "    refine_upper_bound = session.context.get(\"refine_upper_bound\", 3)\n",
    "    rounds = 0\n",
    "    # Maybe we need to use a thinking agent here (Lets test)\n",
    "    while (True):\n",
    "        session.context.set(\"researcher__tool_called\", False)\n",
    "        if first_query:\n",
    "            agent.log_agent_step(session, \"Started research process\", [], print_full_message=False)\n",
    "            message = TemplatedPrompt(\"researcher.query\", \"user\").get_message(\n",
    "                session.context.get_context()\n",
    "            )\n",
    "            first_query = False\n",
    "        else:\n",
    "            agent.log_agent_step(session, \"Started reflecting on the information\", [], print_full_message=False)\n",
    "            message = TemplatedPrompt(\"researcher.reflect\", \"user\").get_message(\n",
    "                session.context.get_context()\n",
    "            )       \n",
    "        agent.conversate(message, session)\n",
    "        rounds += 1 \n",
    "        if not session.context.get(\"researcher__tool_called\") or rounds >= refine_upper_bound:\n",
    "            break\n",
    "\n",
    "system_prompt = SystemPrompt('researcher.system')\n",
    "researcher_llm = Llama4Maverick(system_prompt)\n",
    "toolbox = new_toolbox()\n",
    "toolbox.add_to_toolbox(get_information)\n",
    "agent_researcher = (\n",
    "    create_agent(researcher_llm, \"researcher\")\n",
    "        .with_act(researcher_act_function)\n",
    "        .with_tool_callback(tool_agg)\n",
    "        .with_tools(toolbox)\n",
    "        .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bccbfe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_act_function(agent:Agent, session:Session):\n",
    "    agent.reset_agent_state()\n",
    "    # Compile summary\n",
    "    researched_information = session.context.get_cumulative(\"researched_information\")\n",
    "    dict_to_list = lambda k, v: dict(v, **{\"term\": k})\n",
    "    researched_information = [dict_to_list(k, item) for sublist in researched_information for k, item in sublist.items()]\n",
    "    summaries = [\n",
    "        f\"<TERM>{data.get('term')}</TERM>\\n<SOURCE>{data.get('source')}</SOURCE>\\n<CONTENT>\\n{data.get('llm_summary').get('compressed_text')}</CONTENT>\\n\" \n",
    "        for data in researched_information\n",
    "    ]\n",
    "    session.context.set(\"summary\", '\\n'.join(summaries))\n",
    "    message = TemplatedPrompt(\"report_generator.instruct\", \"user\").get_message(\n",
    "        session.context.get_context()\n",
    "    )\n",
    "    result = agent.conversate(message, session)\n",
    "    session.context.set(\"report\", result.content)\n",
    "    agent.log_agent_step(session, \"Wrote a report\", [result])\n",
    "\n",
    "def post_act_function(agent:Agent, session:Session):\n",
    "    request_uuid = session.session_id\n",
    "    report = session.context.get(\"report\")\n",
    "    with open(f\"results/{request_uuid}.md\", 'w') as f:\n",
    "        f.write(report)\n",
    "    agent.log_agent_step(session, \"Saved the report to a file\", [])\n",
    "    \n",
    "    \n",
    "system_prompt = SystemPrompt('report_generator.system')\n",
    "report_llm = Llama4Maverick(system_prompt)\n",
    "agent_reporter = (\n",
    "    create_agent(report_llm, \"report_generator\")\n",
    "        .with_act(report_act_function)\n",
    "        .with_post_act(post_act_function)\n",
    "        .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd6b56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_deep_research(query, refine_upper_bound=8):\n",
    "    session = Session()\n",
    "    session.context.set(\"user_query\", query)\n",
    "    session.context.set(\"refine_upper_bound\", refine_upper_bound)\n",
    "\n",
    "    # Agent \"chaining\"\n",
    "    agent_expert_maker.act(session)\n",
    "    agent_planner.act(session)\n",
    "    agent_researcher.act(session)\n",
    "    agent_reporter.act(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f3092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 18:07:30,345 - pipelines_logger - INFO - \u001b[92m(expert_maker) has defined the persona\u001b[0m\n",
      "2025-08-04 18:07:32,599 - pipelines_logger - INFO - \u001b[92m(planner) has defined the plan\u001b[0m\n",
      "2025-08-04 18:07:32,599 - pipelines_logger - INFO - \u001b[92m(researcher) Started research process\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent called the search algorithm with ['definition of microservice from reputable sources', 'characteristics and principles of microservices architecture', 'definition and characteristics of nanoservices', 'comparison between microservices and nanoservices', 'expert opinions and case studies on microservices and nanoservices']\n",
      "The agent is searching with definition of microservice from reputable sources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 18:07:43,960 - pipelines_logger - INFO - \u001b[92m(text_compressor) has compressed a text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent is searching with characteristics and principles of microservices architecture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 18:08:03,137 - pipelines_logger - INFO - \u001b[92m(text_compressor) has compressed a text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent is searching with definition and characteristics of nanoservices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 18:08:20,967 - pipelines_logger - INFO - \u001b[92m(text_compressor) has compressed a text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent is searching with comparison between microservices and nanoservices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 18:08:33,901 - pipelines_logger - INFO - \u001b[92m(text_compressor) has compressed a text\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent is searching with expert opinions and case studies on microservices and nanoservices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 18:08:41,215 - pipelines_logger - INFO - \u001b[92m(text_compressor) has compressed a text\u001b[0m\n",
      "2025-08-04 18:08:41,215 - pipelines_logger - INFO - \u001b[92m(researcher) Agent got refined data back\u001b[0m\n",
      "2025-08-04 18:08:46,251 - pipelines_logger - INFO - \u001b[92m(report_generator) Wrote a report\u001b[0m\n",
      "2025-08-04 18:08:46,253 - pipelines_logger - INFO - \u001b[92m(report_generator) Saved the report to a file\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"What defines a Microsservice? I need to be able to discern between `microsservice` and `nanoservices`\"\n",
    "perform_deep_research(query, refine_upper_bound=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe8e85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
